{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76bd80c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Github: https://github.com/swatichauhan08/uber_reviews/blob/main/code.py\n",
    "# Streanlit has been checked and it is working"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "208825a4",
   "metadata": {},
   "source": [
    "#### Q1. Pre-processing: Examine the dataset. ID the columns of interest. Drop special characters, html junk etc. Perform any other preprocessing and text-cleaning activity you think fits this context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7e27cf13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.562\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## setup chunk\n",
    "import time   # to time 'em opns\n",
    "t0 = time.time()    # start timer\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import os\n",
    "import codecs\n",
    "from sklearn import feature_extraction\n",
    "import csv\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "# import mpld3  # conda install -c conda-forge mpld3 \n",
    "t1 = time.time()\n",
    "\n",
    "time.taken = round(t1-t0, 3)\n",
    "print(time.taken)\n",
    "print(\"\\n\")    # print newline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "439da8e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    Category  \\\n",
      "0            DevOps Engineer   \n",
      "1               Data Science   \n",
      "2               Data Science   \n",
      "3               Data Science   \n",
      "4               Data Science   \n",
      "...                      ...   \n",
      "1243             MS Dynamics   \n",
      "1244     Java Cloud Services   \n",
      "1245    Azure Cloud Services   \n",
      "1246  Azure Devops Architect   \n",
      "1247             MS Dynamics   \n",
      "\n",
      "                                                 Resume  Exp (Years)  \\\n",
      "0     technical skills key skills ms technology net ...          NaN   \n",
      "1     education details  mca   ymcaust  faridabad  h...          NaN   \n",
      "2     skills c basics iot python matlab data science...          NaN   \n",
      "3     skills  r  python  sap hana  tableau  sap hana...          NaN   \n",
      "4     skills  python  tableau  data visualization  r...          NaN   \n",
      "...                                                 ...          ...   \n",
      "1243  core competencies  ant  maven  git  bitbucket ...          NaN   \n",
      "1244  technical skills key skills ms technology net ...          NaN   \n",
      "1245  core skills  project  program management  agil...          NaN   \n",
      "1246  core skills  project  program management  agil...          NaN   \n",
      "1247  technical skills key skills ms technology net ...          NaN   \n",
      "\n",
      "                     Name Current Salary  Expected Salary  Offered Salary  \\\n",
      "0           Avinash Joshi        2650000          3090000         2960000   \n",
      "1         Karishma Neving         684000          1240000         1100000   \n",
      "2             Aaron Miles         603000          1120000         1100000   \n",
      "3              Dhruv Jain         745000          1180000         1080000   \n",
      "4           Grace Mallory         762000          1270000         1140000   \n",
      "...                   ...            ...              ...             ...   \n",
      "1243        Kaviyarasan G        1100000          1700000         1700000   \n",
      "1244     Divya Surishetty         985000          1200000         1200000   \n",
      "1245         Madhu Sudhan        1260000          1700000         1700000   \n",
      "1246    Padmaja Samantara        2525000          3200000         3200000   \n",
      "1247  Sai Krishna Bathini         460000           700000          700000   \n",
      "\n",
      "      Age Current Location Offered Location Offer Status  \n",
      "0      43             Pune             Pune     Accepted  \n",
      "1      28        Hyderabad        Hyderabad     Declined  \n",
      "2      25          Gurgaon          Gurgaon     Accepted  \n",
      "3      25        Bangalore        Bangalore     Accepted  \n",
      "4      26          Gurgaon          Gurgaon     Declined  \n",
      "...   ...              ...              ...          ...  \n",
      "1243   35          Chennai          Chennai     Declined  \n",
      "1244   31        Hyderabad        Hyderabad     Accepted  \n",
      "1245   27        Hyderabad        Hyderabad     Accepted  \n",
      "1246   31        Bangalore        Bangalore     Accepted  \n",
      "1247   26        Hyderabad        Hyderabad     Accepted  \n",
      "\n",
      "[1248 rows x 11 columns]\n"
     ]
    }
   ],
   "source": [
    "# importing the csv file into the dataframe\n",
    "datafile = pd.read_csv(r'//Users/Dell/Desktop/FP1 Project - All Files/Final_Applicants_list.csv')\n",
    "Candidate_Resume = pd.DataFrame(datafile, columns= ['Category','Resume','Exp (Years)','Name','Current Salary','Expected Salary','Offered Salary','Age','Current Location','Offered Location','Offer Status'])\n",
    "print(Candidate_Resume)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3b9add67",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dropping unnecessary columns\n",
    "Candidate_Resume.drop('Offer Status', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "78cd35bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Category</th>\n",
       "      <th>Resume</th>\n",
       "      <th>Exp (Years)</th>\n",
       "      <th>Name</th>\n",
       "      <th>Current Salary</th>\n",
       "      <th>Expected Salary</th>\n",
       "      <th>Offered Salary</th>\n",
       "      <th>Age</th>\n",
       "      <th>Current Location</th>\n",
       "      <th>Offered Location</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>devops engineer</td>\n",
       "      <td>technical skills key skills ms technology net ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Avinash Joshi</td>\n",
       "      <td>2650000</td>\n",
       "      <td>3090000</td>\n",
       "      <td>2960000</td>\n",
       "      <td>43</td>\n",
       "      <td>Pune</td>\n",
       "      <td>Pune</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>data science</td>\n",
       "      <td>education details  mca   ymcaust  faridabad  h...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Karishma Neving</td>\n",
       "      <td>684000</td>\n",
       "      <td>1240000</td>\n",
       "      <td>1100000</td>\n",
       "      <td>28</td>\n",
       "      <td>Hyderabad</td>\n",
       "      <td>Hyderabad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>data science</td>\n",
       "      <td>skills c basics iot python matlab data science...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Aaron Miles</td>\n",
       "      <td>603000</td>\n",
       "      <td>1120000</td>\n",
       "      <td>1100000</td>\n",
       "      <td>25</td>\n",
       "      <td>Gurgaon</td>\n",
       "      <td>Gurgaon</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>data science</td>\n",
       "      <td>skills  r  python  sap hana  tableau  sap hana...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Dhruv Jain</td>\n",
       "      <td>745000</td>\n",
       "      <td>1180000</td>\n",
       "      <td>1080000</td>\n",
       "      <td>25</td>\n",
       "      <td>Bangalore</td>\n",
       "      <td>Bangalore</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>data science</td>\n",
       "      <td>skills  python  tableau  data visualization  r...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Grace Mallory</td>\n",
       "      <td>762000</td>\n",
       "      <td>1270000</td>\n",
       "      <td>1140000</td>\n",
       "      <td>26</td>\n",
       "      <td>Gurgaon</td>\n",
       "      <td>Gurgaon</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1243</th>\n",
       "      <td>ms dynamics</td>\n",
       "      <td>core competencies  ant  maven  git  bitbucket ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Kaviyarasan G</td>\n",
       "      <td>1100000</td>\n",
       "      <td>1700000</td>\n",
       "      <td>1700000</td>\n",
       "      <td>35</td>\n",
       "      <td>Chennai</td>\n",
       "      <td>Chennai</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1244</th>\n",
       "      <td>java cloud services</td>\n",
       "      <td>technical skills key skills ms technology net ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Divya Surishetty</td>\n",
       "      <td>985000</td>\n",
       "      <td>1200000</td>\n",
       "      <td>1200000</td>\n",
       "      <td>31</td>\n",
       "      <td>Hyderabad</td>\n",
       "      <td>Hyderabad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1245</th>\n",
       "      <td>azure cloud services</td>\n",
       "      <td>core skills  project  program management  agil...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Madhu Sudhan</td>\n",
       "      <td>1260000</td>\n",
       "      <td>1700000</td>\n",
       "      <td>1700000</td>\n",
       "      <td>27</td>\n",
       "      <td>Hyderabad</td>\n",
       "      <td>Hyderabad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1246</th>\n",
       "      <td>azure devops architect</td>\n",
       "      <td>core skills  project  program management  agil...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Padmaja Samantara</td>\n",
       "      <td>2525000</td>\n",
       "      <td>3200000</td>\n",
       "      <td>3200000</td>\n",
       "      <td>31</td>\n",
       "      <td>Bangalore</td>\n",
       "      <td>Bangalore</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1247</th>\n",
       "      <td>ms dynamics</td>\n",
       "      <td>technical skills key skills ms technology net ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Sai Krishna Bathini</td>\n",
       "      <td>460000</td>\n",
       "      <td>700000</td>\n",
       "      <td>700000</td>\n",
       "      <td>26</td>\n",
       "      <td>Hyderabad</td>\n",
       "      <td>Hyderabad</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1248 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                    Category  \\\n",
       "0            devops engineer   \n",
       "1               data science   \n",
       "2               data science   \n",
       "3               data science   \n",
       "4               data science   \n",
       "...                      ...   \n",
       "1243             ms dynamics   \n",
       "1244     java cloud services   \n",
       "1245    azure cloud services   \n",
       "1246  azure devops architect   \n",
       "1247             ms dynamics   \n",
       "\n",
       "                                                 Resume  Exp (Years)  \\\n",
       "0     technical skills key skills ms technology net ...          NaN   \n",
       "1     education details  mca   ymcaust  faridabad  h...          NaN   \n",
       "2     skills c basics iot python matlab data science...          NaN   \n",
       "3     skills  r  python  sap hana  tableau  sap hana...          NaN   \n",
       "4     skills  python  tableau  data visualization  r...          NaN   \n",
       "...                                                 ...          ...   \n",
       "1243  core competencies  ant  maven  git  bitbucket ...          NaN   \n",
       "1244  technical skills key skills ms technology net ...          NaN   \n",
       "1245  core skills  project  program management  agil...          NaN   \n",
       "1246  core skills  project  program management  agil...          NaN   \n",
       "1247  technical skills key skills ms technology net ...          NaN   \n",
       "\n",
       "                     Name Current Salary  Expected Salary  Offered Salary  \\\n",
       "0           Avinash Joshi        2650000          3090000         2960000   \n",
       "1         Karishma Neving         684000          1240000         1100000   \n",
       "2             Aaron Miles         603000          1120000         1100000   \n",
       "3              Dhruv Jain         745000          1180000         1080000   \n",
       "4           Grace Mallory         762000          1270000         1140000   \n",
       "...                   ...            ...              ...             ...   \n",
       "1243        Kaviyarasan G        1100000          1700000         1700000   \n",
       "1244     Divya Surishetty         985000          1200000         1200000   \n",
       "1245         Madhu Sudhan        1260000          1700000         1700000   \n",
       "1246    Padmaja Samantara        2525000          3200000         3200000   \n",
       "1247  Sai Krishna Bathini         460000           700000          700000   \n",
       "\n",
       "      Age Current Location Offered Location  \n",
       "0      43             Pune             Pune  \n",
       "1      28        Hyderabad        Hyderabad  \n",
       "2      25          Gurgaon          Gurgaon  \n",
       "3      25        Bangalore        Bangalore  \n",
       "4      26          Gurgaon          Gurgaon  \n",
       "...   ...              ...              ...  \n",
       "1243   35          Chennai          Chennai  \n",
       "1244   31        Hyderabad        Hyderabad  \n",
       "1245   27        Hyderabad        Hyderabad  \n",
       "1246   31        Bangalore        Bangalore  \n",
       "1247   26        Hyderabad        Hyderabad  \n",
       "\n",
       "[1248 rows x 10 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#converting the data into lowercase for analysis\n",
    "Candidate_Resume['Category'] = Candidate_Resume['Category'].str.lower()\n",
    "Candidate_Resume['Resume'] = Candidate_Resume['Resume'].str.lower()\n",
    "Candidate_Resume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "5114ae79",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Rating</th>\n",
       "      <th>Review</th>\n",
       "      <th>Date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>dishonest and disgusting</td>\n",
       "      <td>1</td>\n",
       "      <td>for half an hour we tried every uber service t...</td>\n",
       "      <td>29-12-2020 01:14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>free offer</td>\n",
       "      <td>2</td>\n",
       "      <td>if im not eligible for the offer stop floodin...</td>\n",
       "      <td>01-01-2021 23:17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>inaccurate</td>\n",
       "      <td>2</td>\n",
       "      <td>consistently inaccurate uber eats eta and the ...</td>\n",
       "      <td>15-01-2021 23:38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>bad</td>\n",
       "      <td>1</td>\n",
       "      <td>i had my rides canceled back to back they then...</td>\n",
       "      <td>08-12-2020 01:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>double charged me for an order</td>\n",
       "      <td>1</td>\n",
       "      <td>two of the same orders was added by accident t...</td>\n",
       "      <td>15-12-2020 04:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>485</th>\n",
       "      <td>uber</td>\n",
       "      <td>5</td>\n",
       "      <td>perdí mi cuenta no la puedo recuperar la use e...</td>\n",
       "      <td>16-01-2021 02:39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>486</th>\n",
       "      <td>crap crap crap</td>\n",
       "      <td>1</td>\n",
       "      <td>still the same i was forced to use it in colom...</td>\n",
       "      <td>23-12-2020 00:15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>487</th>\n",
       "      <td>sleeping drivers</td>\n",
       "      <td>1</td>\n",
       "      <td>it is a 30 minute commute from my household to...</td>\n",
       "      <td>16-12-2020 19:10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>488</th>\n",
       "      <td>bad design re offer code redemption and issue ...</td>\n",
       "      <td>1</td>\n",
       "      <td>was sent a 30 off uber eats i thought about pl...</td>\n",
       "      <td>25-11-2020 23:06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>489</th>\n",
       "      <td>im sick of uber</td>\n",
       "      <td>1</td>\n",
       "      <td>at first uber was okay but than things started...</td>\n",
       "      <td>05-12-2020 12:19</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>490 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 Title  Rating  \\\n",
       "0                             dishonest and disgusting       1   \n",
       "1                                           free offer       2   \n",
       "2                                           inaccurate       2   \n",
       "3                                                  bad       1   \n",
       "4                       double charged me for an order       1   \n",
       "..                                                 ...     ...   \n",
       "485                                               uber       5   \n",
       "486                                     crap crap crap       1   \n",
       "487                                   sleeping drivers       1   \n",
       "488  bad design re offer code redemption and issue ...       1   \n",
       "489                                   im sick of uber       1   \n",
       "\n",
       "                                                Review              Date  \n",
       "0    for half an hour we tried every uber service t...  29-12-2020 01:14  \n",
       "1    if im not eligible for the offer stop floodin...  01-01-2021 23:17  \n",
       "2    consistently inaccurate uber eats eta and the ...  15-01-2021 23:38  \n",
       "3    i had my rides canceled back to back they then...  08-12-2020 01:01  \n",
       "4    two of the same orders was added by accident t...  15-12-2020 04:02  \n",
       "..                                                 ...               ...  \n",
       "485  perdí mi cuenta no la puedo recuperar la use e...  16-01-2021 02:39  \n",
       "486  still the same i was forced to use it in colom...  23-12-2020 00:15  \n",
       "487  it is a 30 minute commute from my household to...  16-12-2020 19:10  \n",
       "488  was sent a 30 off uber eats i thought about pl...  25-11-2020 23:06  \n",
       "489  at first uber was okay but than things started...  05-12-2020 12:19  \n",
       "\n",
       "[490 rows x 4 columns]"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# creating the function for punctuation removal\n",
    "def remove_punctuations(text):\n",
    "    for char in string.punctuation:\n",
    "        text = text.replace(char, '')\n",
    "    return text\n",
    "\n",
    "uber_reviews['Title'] = uber_reviews['Title'].apply(remove_punctuations)\n",
    "uber_reviews['Review'] = uber_reviews['Review'].apply(remove_punctuations)\n",
    "uber_reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "37c1e945",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Rating</th>\n",
       "      <th>Review</th>\n",
       "      <th>Date</th>\n",
       "      <th>Title_nostop</th>\n",
       "      <th>Review_nostop</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>dishonest and disgusting</td>\n",
       "      <td>1</td>\n",
       "      <td>for half an hour we tried every uber service t...</td>\n",
       "      <td>29-12-2020 01:14</td>\n",
       "      <td>dishonest disgusting</td>\n",
       "      <td>half hour tried every service take short trip ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>free offer</td>\n",
       "      <td>2</td>\n",
       "      <td>if im not eligible for the offer stop floodin...</td>\n",
       "      <td>01-01-2021 23:17</td>\n",
       "      <td>free offer</td>\n",
       "      <td>eligible offer stop flooding email false infor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>inaccurate</td>\n",
       "      <td>2</td>\n",
       "      <td>consistently inaccurate uber eats eta and the ...</td>\n",
       "      <td>15-01-2021 23:38</td>\n",
       "      <td>inaccurate</td>\n",
       "      <td>consistently inaccurate eats eta food menus of...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>bad</td>\n",
       "      <td>1</td>\n",
       "      <td>i had my rides canceled back to back they then...</td>\n",
       "      <td>08-12-2020 01:01</td>\n",
       "      <td>bad</td>\n",
       "      <td>rides canceled back back still charged account...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>double charged me for an order</td>\n",
       "      <td>1</td>\n",
       "      <td>two of the same orders was added by accident t...</td>\n",
       "      <td>15-12-2020 04:02</td>\n",
       "      <td>double charged order</td>\n",
       "      <td>two orders added accident restaurant tried rem...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>485</th>\n",
       "      <td>uber</td>\n",
       "      <td>5</td>\n",
       "      <td>perdí mi cuenta no la puedo recuperar la use e...</td>\n",
       "      <td>16-01-2021 02:39</td>\n",
       "      <td></td>\n",
       "      <td>perdí mi cuenta la puedo recuperar la use el m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>486</th>\n",
       "      <td>crap crap crap</td>\n",
       "      <td>1</td>\n",
       "      <td>still the same i was forced to use it in colom...</td>\n",
       "      <td>23-12-2020 00:15</td>\n",
       "      <td>crap crap crap</td>\n",
       "      <td>still forced use colombia since know ride shar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>487</th>\n",
       "      <td>sleeping drivers</td>\n",
       "      <td>1</td>\n",
       "      <td>it is a 30 minute commute from my household to...</td>\n",
       "      <td>16-12-2020 19:10</td>\n",
       "      <td>sleeping drivers</td>\n",
       "      <td>minute commute household worksite tend use get...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>488</th>\n",
       "      <td>bad design re offer code redemption and issue ...</td>\n",
       "      <td>1</td>\n",
       "      <td>was sent a 30 off uber eats i thought about pl...</td>\n",
       "      <td>25-11-2020 23:06</td>\n",
       "      <td>bad design offer code redemption issue resolution</td>\n",
       "      <td>sent eats thought placing order applied code c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>489</th>\n",
       "      <td>im sick of uber</td>\n",
       "      <td>1</td>\n",
       "      <td>at first uber was okay but than things started...</td>\n",
       "      <td>05-12-2020 12:19</td>\n",
       "      <td>sick</td>\n",
       "      <td>first okay things started happening like drive...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>490 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 Title  Rating  \\\n",
       "0                             dishonest and disgusting       1   \n",
       "1                                           free offer       2   \n",
       "2                                           inaccurate       2   \n",
       "3                                                  bad       1   \n",
       "4                       double charged me for an order       1   \n",
       "..                                                 ...     ...   \n",
       "485                                               uber       5   \n",
       "486                                     crap crap crap       1   \n",
       "487                                   sleeping drivers       1   \n",
       "488  bad design re offer code redemption and issue ...       1   \n",
       "489                                   im sick of uber       1   \n",
       "\n",
       "                                                Review              Date  \\\n",
       "0    for half an hour we tried every uber service t...  29-12-2020 01:14   \n",
       "1    if im not eligible for the offer stop floodin...  01-01-2021 23:17   \n",
       "2    consistently inaccurate uber eats eta and the ...  15-01-2021 23:38   \n",
       "3    i had my rides canceled back to back they then...  08-12-2020 01:01   \n",
       "4    two of the same orders was added by accident t...  15-12-2020 04:02   \n",
       "..                                                 ...               ...   \n",
       "485  perdí mi cuenta no la puedo recuperar la use e...  16-01-2021 02:39   \n",
       "486  still the same i was forced to use it in colom...  23-12-2020 00:15   \n",
       "487  it is a 30 minute commute from my household to...  16-12-2020 19:10   \n",
       "488  was sent a 30 off uber eats i thought about pl...  25-11-2020 23:06   \n",
       "489  at first uber was okay but than things started...  05-12-2020 12:19   \n",
       "\n",
       "                                          Title_nostop  \\\n",
       "0                                 dishonest disgusting   \n",
       "1                                           free offer   \n",
       "2                                           inaccurate   \n",
       "3                                                  bad   \n",
       "4                                 double charged order   \n",
       "..                                                 ...   \n",
       "485                                                      \n",
       "486                                     crap crap crap   \n",
       "487                                   sleeping drivers   \n",
       "488  bad design offer code redemption issue resolution   \n",
       "489                                               sick   \n",
       "\n",
       "                                         Review_nostop  \n",
       "0    half hour tried every service take short trip ...  \n",
       "1    eligible offer stop flooding email false infor...  \n",
       "2    consistently inaccurate eats eta food menus of...  \n",
       "3    rides canceled back back still charged account...  \n",
       "4    two orders added accident restaurant tried rem...  \n",
       "..                                                 ...  \n",
       "485  perdí mi cuenta la puedo recuperar la use el m...  \n",
       "486  still forced use colombia since know ride shar...  \n",
       "487  minute commute household worksite tend use get...  \n",
       "488  sent eats thought placing order applied code c...  \n",
       "489  first okay things started happening like drive...  \n",
       "\n",
       "[490 rows x 6 columns]"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# working with the corpus to remove the stopwords\n",
    "\n",
    "stop = stopwords.words('english')\n",
    "guided_list = ['im','uber','10','20','30']\n",
    "stop_extended = stop + guided_list\n",
    "uber_reviews['Title_nostop'] = uber_reviews['Title'].apply(lambda x: ' '.join([word for word in x.split() if word not in stop_extended]))\n",
    "uber_reviews['Review_nostop'] = uber_reviews['Review'].apply(lambda x: ' '.join([word for word in x.split() if word not in stop_extended]))\n",
    "uber_reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "32a58187",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "489"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(uber_reviews['Title_nostop'].unique())\n",
    "len(uber_reviews['Review_nostop'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "6d142313",
   "metadata": {},
   "outputs": [],
   "source": [
    "uber_reviews.drop('Title', axis=1, inplace=True)\n",
    "uber_reviews.drop('Review', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "182b4200",
   "metadata": {},
   "source": [
    "#### 2. Basic text and sentiment analysis: You could choose to run these to get a feel for the data. What are people in general saying, etc. What is the distribution of valence across the corpus and within documents. Use bigrams, COGs, wordclouds, phrases etc if you want. May help to randomly read through a few reviews to get a feel, as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "c9b68f28",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'neg': 0.0, 'neu': 0.913, 'pos': 0.087, 'compound': 0.2732}"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "# see what kinda output VADER yields on 1 doc first\n",
    "vs0 = analyzer.polarity_scores(uber_reviews['Review_nostop'].iloc[0]); vs0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "d5b397ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['half hour tried every service take short trip falsely advertised dropoffs certain times deliver finally downloded lyft immediately got car two minutes away']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/Dell/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'neg': 0.0, 'neu': 0.913, 'pos': 0.087, 'compound': 0.2732}"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk \n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import sent_tokenize\n",
    "uber_sents_list = sent_tokenize(uber_reviews['Review_nostop'].iloc[0])\n",
    "print(uber_sents_list[:5])  # view first five sents\n",
    "\n",
    "vs0_sent = analyzer.polarity_scores(uber_sents_list[0]); vs0_sent  # aha. works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "0848fef5",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'0'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m<timed exec>:1\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/core/series.py:958\u001b[0m, in \u001b[0;36mSeries.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    955\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_values[key]\n\u001b[1;32m    957\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m key_is_scalar:\n\u001b[0;32m--> 958\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_value\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    960\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_hashable(key):\n\u001b[1;32m    961\u001b[0m     \u001b[38;5;66;03m# Otherwise index.get_value will raise InvalidIndexError\u001b[39;00m\n\u001b[1;32m    962\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    963\u001b[0m         \u001b[38;5;66;03m# For labels that don't resolve as scalars like tuples and frozensets\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/core/series.py:1069\u001b[0m, in \u001b[0;36mSeries._get_value\u001b[0;34m(self, label, takeable)\u001b[0m\n\u001b[1;32m   1066\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_values[label]\n\u001b[1;32m   1068\u001b[0m \u001b[38;5;66;03m# Similar to Index.get_value, but we do not fall back to positional\u001b[39;00m\n\u001b[0;32m-> 1069\u001b[0m loc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1070\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindex\u001b[38;5;241m.\u001b[39m_get_values_for_loc(\u001b[38;5;28mself\u001b[39m, loc, label)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/core/indexes/range.py:389\u001b[0m, in \u001b[0;36mRangeIndex.get_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m    387\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[1;32m    388\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n\u001b[0;32m--> 389\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key)\n\u001b[1;32m    390\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mget_loc(key, method\u001b[38;5;241m=\u001b[39mmethod, tolerance\u001b[38;5;241m=\u001b[39mtolerance)\n",
      "\u001b[0;31mKeyError\u001b[0m: '0'"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = uber_reviews['Review_nostop']\n",
    "uber_review = data.copy()\n",
    "\n",
    "doc0_df = []\n",
    "# define unit func to process one doc\n",
    "def vader_unit_func(doc0):\n",
    "    sents_list0 = sent_tokenize(doc0)\n",
    "    vs_doc0 = []\n",
    "    sent_df = []\n",
    "    for i in range(len(sents_list0)):\n",
    "        vs_sent0 = analyzer.polarity_scores(sents_list0[i])\n",
    "        vs_doc0.append(vs_sent0)\n",
    "        sent_ind.append(i)\n",
    "        \n",
    "    doc0_df = pd.DataFrame(vs_doc0)\n",
    "    doc0_df.insert(0, 'sent_index', sent_ind)  # insert sent index\n",
    "    doc0_df.insert(doc0_df.shape[1], 'sentence', sents_list0)\n",
    "    return(doc0_df)\n",
    "\n",
    "%time doc0_df = vader_unit_func(uber_review['0'])\n",
    "doc0_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "56317824",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      half hour tried every service take short trip ...\n",
       "1      eligible offer stop flooding email false infor...\n",
       "2      consistently inaccurate eats eta food menus of...\n",
       "3      rides canceled back back still charged account...\n",
       "4      two orders added accident restaurant tried rem...\n",
       "                             ...                        \n",
       "485    perdí mi cuenta la puedo recuperar la use el m...\n",
       "486    still forced use colombia since know ride shar...\n",
       "487    minute commute household worksite tend use get...\n",
       "488    sent eats thought placing order applied code c...\n",
       "489    first okay things started happening like drive...\n",
       "Name: Review_nostop, Length: 490, dtype: object"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uber_review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "484233f8",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Series' object has no attribute 'text'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "File \u001b[0;32m<timed exec>:1\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "Input \u001b[0;32mIn [58]\u001b[0m, in \u001b[0;36mvader_wrap_func\u001b[0;34m(corpus0)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# apply unit-func to each doc & loop over all docs\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i1 \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(corpus0)):\n\u001b[0;32m---> 13\u001b[0m     doc0 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(\u001b[43mcorpus0\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtext\u001b[49m\u001b[38;5;241m.\u001b[39miloc[i1])\n\u001b[1;32m     14\u001b[0m     vs_doc_df \u001b[38;5;241m=\u001b[39m vader_unit_func(doc0)  \u001b[38;5;66;03m# applying unit-func\u001b[39;00m\n\u001b[1;32m     15\u001b[0m     vs_doc_df\u001b[38;5;241m.\u001b[39minsert(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdoc_index\u001b[39m\u001b[38;5;124m'\u001b[39m, i1)  \u001b[38;5;66;03m# inserting doc index\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/core/generic.py:5575\u001b[0m, in \u001b[0;36mNDFrame.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   5568\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   5569\u001b[0m     name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_internal_names_set\n\u001b[1;32m   5570\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_metadata\n\u001b[1;32m   5571\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_accessors\n\u001b[1;32m   5572\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_info_axis\u001b[38;5;241m.\u001b[39m_can_hold_identifiers_and_holds_name(name)\n\u001b[1;32m   5573\u001b[0m ):\n\u001b[1;32m   5574\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m[name]\n\u001b[0;32m-> 5575\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mobject\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__getattribute__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Series' object has no attribute 'text'"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'uber_vs_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [58]\u001b[0m, in \u001b[0;36m<cell line: 22>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# test-drive wrapper func\u001b[39;00m\n\u001b[1;32m     21\u001b[0m get_ipython()\u001b[38;5;241m.\u001b[39mrun_line_magic(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtime\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124muber_vs_df = vader_wrap_func(uber_review)\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 22\u001b[0m \u001b[43muber_vs_df\u001b[49m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'uber_vs_df' is not defined"
     ]
    }
   ],
   "source": [
    "# define wrapper func\n",
    "def vader_wrap_func(corpus0):\n",
    "    \n",
    "    # use ifinstance() to check & convert input to DF\n",
    "    if isinstance(corpus0, list):\n",
    "        corpus0 = pd.DataFrame({'text':corpus0})\n",
    "    \n",
    "    # define empty DF to concat unit func output to\n",
    "    vs_df = pd.DataFrame(columns=['doc_index', 'sent_index', 'neg', 'neu', 'pos', 'compound', 'sentence'])    \n",
    "    \n",
    "    # apply unit-func to each doc & loop over all docs\n",
    "    for i1 in range(len(corpus0)):\n",
    "        doc0 = str(corpus0.text.iloc[i1])\n",
    "        vs_doc_df = vader_unit_func(doc0)  # applying unit-func\n",
    "        vs_doc_df.insert(0, 'doc_index', i1)  # inserting doc index\n",
    "        vs_df = pd.concat([vs_df, vs_doc_df], axis=0)\n",
    "        \n",
    "    return(vs_df)\n",
    "\n",
    "# test-drive wrapper func\n",
    "%time uber_vs_df = vader_wrap_func(uber_review)    \n",
    "uber_vs_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9085015",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Sentiment analysis proves negative interactions since the data is largely complaints against the drivers which involes wai times and other issues"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53bd8913",
   "metadata": {},
   "source": [
    "#### 3. Feature construction and extraction: Recall that client wants to know how text features impact overall ratings. So think about what features in text might impact rating. 1. Full DTM? Sentiment scores? Valence shifted sentiment? Your call on whether or not this applies to this context. 2. Topic scores of each document after running a latent topic model (LTM)? Remember to interpret and label topics if you decide to use LTMs. 3. Number of matches with some custom lexicon or wordlist you built? You could build a lexicon or wordlist of terms used by Uber users (e.g. driver, fare, payment etc) which maynot be caught in the sentiment lexicon and use counts of matched tokens as a feature. 4. Any NLP based features? parts of speech? named entities? etc. 5. Any other feature you think may be relevant in this context. For instance, will the Date field offer useful info? For example, do complaints spike on say, weekends versus weekdays? Rush hour versus other times?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "ebdabb7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load nltk's English stopwords as variable called 'stopwords'\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "# load nltk's SnowballStemmer as variabled 'stemmer'\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "\n",
    "## here I define a tokenizer and stemmer which returns the set of stems in the text that it is passed\n",
    "def tokenize_and_stem(text):\n",
    "    # first tokenize by sentence, then by word to ensure that punctuation is caught as it's own token\n",
    "    tokens = [word for sent in nltk.sent_tokenize(text) for word in nltk.word_tokenize(sent)]\n",
    "    filtered_tokens = []\n",
    "    # filter out any tokens not containing letters (e.g., numeric tokens, raw punctuation) using regex\n",
    "    for token in tokens:\n",
    "        if re.search('[a-zA-Z]', token):\n",
    "            filtered_tokens.append(token)\n",
    "    stems = [stemmer.stem(t) for t in filtered_tokens]\n",
    "    return stems\n",
    "\n",
    "\n",
    "def tokenize_only(text):\n",
    "    # first tokenize by sentence, then by word to ensure that punctuation is caught as it's own token\n",
    "    tokens = [word.lower() for sent in nltk.sent_tokenize(text) for word in nltk.word_tokenize(sent)]\n",
    "    filtered_tokens = []\n",
    "    # filter out any tokens not containing letters (e.g., numeric tokens, raw punctuation)\n",
    "    for token in tokens:\n",
    "        if re.search('[a-zA-Z]', token):\n",
    "            filtered_tokens.append(token)\n",
    "    return filtered_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "31451b13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.306\n"
     ]
    }
   ],
   "source": [
    "# Use above funcs to iterate over the list of synopses to create two vocabularies: one stemmed and one only tokenized. \n",
    "totalvocab_stemmed = []\n",
    "totalvocab_tokenized = []\n",
    "uber_review_ = uber_review.tolist()\n",
    "\n",
    "t0 = time.time()\n",
    "for i in uber_review_:\n",
    "    \n",
    "    # doing both toknz & stemming\n",
    "    allwords_stemmed = tokenize_and_stem(i)\n",
    "    totalvocab_stemmed.extend(allwords_stemmed)\n",
    "    \n",
    "    # doing toknz only\n",
    "    allwords_tokenized = tokenize_only(i)\n",
    "    totalvocab_tokenized.extend(allwords_tokenized)\n",
    "    \n",
    "t1 = time.time()\n",
    "print(round(t1-t0, 3))    # 0.2 s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "9131d655",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>half</th>\n",
       "      <td>half</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hour</th>\n",
       "      <td>hour</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tri</th>\n",
       "      <td>tried</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>everi</th>\n",
       "      <td>every</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>uber</th>\n",
       "      <td>uber</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>head</th>\n",
       "      <td>head</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>charg</th>\n",
       "      <td>charge</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>done</th>\n",
       "      <td>done</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>use</th>\n",
       "      <td>using</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>uber</th>\n",
       "      <td>uber</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>15307 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        words\n",
       "half     half\n",
       "hour     hour\n",
       "tri     tried\n",
       "everi   every\n",
       "uber     uber\n",
       "...       ...\n",
       "head     head\n",
       "charg  charge\n",
       "done     done\n",
       "use     using\n",
       "uber     uber\n",
       "\n",
       "[15307 rows x 1 columns]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## create a pandas DataFrame with the stemmed vocabulary as the index and the tokenized words as the column\n",
    "vocab_frame = pd.DataFrame({'words': totalvocab_tokenized}, index = totalvocab_stemmed)\n",
    "vocab_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "13563083",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(490, 45164)\n"
     ]
    }
   ],
   "source": [
    "## Tf-idf and document similarity\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(max_df=1, # max proportion of docs word is present in\n",
    "\t\t\t\t   max_features=200000,\n",
    "                                   min_df=0, \n",
    "\t\t\t\t   stop_words='english',\n",
    "                                   use_idf=True, \n",
    "\t\t\t\t   tokenizer=tokenize_and_stem, \n",
    "\t\t\t\t   ngram_range=(1,5))\n",
    "\n",
    "# note magic cmd %time\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(uber_review)    # 6.05 secs\n",
    "\n",
    "print(tfidf_matrix.shape)    # dimns of the tfidf matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "5fbabba2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Dell/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['10kyear',\n",
       " '10kyear liter',\n",
       " '10kyear liter everywher',\n",
       " '10kyear liter everywher issu',\n",
       " '10kyear liter everywher issu can\\x92t',\n",
       " '10min',\n",
       " '10min driver',\n",
       " '10min driver drove',\n",
       " '10min driver drove stop',\n",
       " '10min driver drove stop cancel',\n",
       " '10\\x97said',\n",
       " '10\\x97said estim',\n",
       " '10\\x97said estim await',\n",
       " '10\\x97said estim await driver',\n",
       " '10\\x97said estim await driver pick',\n",
       " '14th',\n",
       " '14th trip',\n",
       " '14th trip steal',\n",
       " '14th trip steal custom',\n",
       " '14th trip steal custom money']"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "terms = tfidf_vectorizer.get_feature_names()\n",
    "terms[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "ec6002f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'scipy.sparse.csr.csr_matrix'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "matrix([[0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(type(tfidf_matrix))\n",
    "\n",
    "tfidf_matrix.todense()[:5,:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "b6ad782d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['10kyear',\n",
       " '10kyear liter',\n",
       " '10kyear liter everywher',\n",
       " '10kyear liter everywher issu',\n",
       " '10kyear liter everywher issu can\\x92t',\n",
       " '10min',\n",
       " '10min driver',\n",
       " '10min driver drove',\n",
       " '10min driver drove stop',\n",
       " '10min driver drove stop cancel',\n",
       " '10\\x97said',\n",
       " '10\\x97said estim',\n",
       " '10\\x97said estim await',\n",
       " '10\\x97said estim await driver',\n",
       " '10\\x97said estim await driver pick',\n",
       " '14th',\n",
       " '14th trip',\n",
       " '14th trip steal',\n",
       " '14th trip steal custom',\n",
       " '14th trip steal custom money']"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "terms = tfidf_vectorizer.get_feature_names()\n",
    "terms[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebbea534",
   "metadata": {},
   "source": [
    "#### Supervised Learning phase: Rubber meets road now. Run a regression (or classification, if you prefer) of review ratings against the text features you have collected in the previous step. Use any regression or classification method you want to. OLS regression is easy to run and interpret, and is hence preferable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "6e2c18da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: xgboost in /Users/Dell/opt/anaconda3/lib/python3.9/site-packages (1.6.1)\n",
      "Requirement already satisfied: scipy in /Users/Dell/opt/anaconda3/lib/python3.9/site-packages (from xgboost) (1.7.3)\n",
      "Requirement already satisfied: numpy in /Users/Dell/opt/anaconda3/lib/python3.9/site-packages (from xgboost) (1.21.5)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "af03074d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## setup chunk\n",
    "from sklearn import model_selection, preprocessing, linear_model, naive_bayes, metrics, svm\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn import ensemble\n",
    "\n",
    "import pandas, xgboost, numpy, string  # , textblob !pip install textblob\n",
    "import csv,re,nltk\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "3648d9c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>half hour tried every service take short trip ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>eligible offer stop flooding email false infor...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>consistently inaccurate eats eta food menus of...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>rides canceled back back still charged account...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>two orders added accident restaurant tried rem...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>even offer promotions one really wants pick ch...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>hate app canceled ride aint even give money b...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>its good times find drivers gps always miss t...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>customer support cant call representative hor...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>makes difficult make account seemingly impossi...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  label\n",
       "0  half hour tried every service take short trip ...      1\n",
       "1  eligible offer stop flooding email false infor...      2\n",
       "2  consistently inaccurate eats eta food menus of...      2\n",
       "3  rides canceled back back still charged account...      1\n",
       "4  two orders added accident restaurant tried rem...      1\n",
       "5  even offer promotions one really wants pick ch...      2\n",
       "6  hate app canceled ride aint even give money b...      1\n",
       "7  its good times find drivers gps always miss t...      3\n",
       "8  customer support cant call representative hor...      1\n",
       "9  makes difficult make account seemingly impossi...      1"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read data in\n",
    "labels, texts = [], []\n",
    "for i in uber_review:\n",
    "    labels.append(i[0])\n",
    "    texts.append(i[1])\n",
    "\n",
    "# build panda DF to house the data\n",
    "trainDF = pandas.DataFrame()\n",
    "trainDF['text'] = uber_review\n",
    "trainDF['label'] = uber_reviews['Rating']\n",
    "trainDF.iloc[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "b7111e7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the DF into training and validation datasets \n",
    "train_x, valid_x, train_y, valid_y = model_selection.train_test_split(trainDF['text'], trainDF['label'])\n",
    "\n",
    "# label encode the target variable into 0/1\n",
    "encoder = preprocessing.LabelEncoder()\n",
    "train_y = encoder.fit_transform(train_y)\n",
    "valid_y = encoder.fit_transform(valid_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "ed7b8f23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.05 secs\n",
      "(367, 3323)\n",
      "(123, 3323)\n"
     ]
    }
   ],
   "source": [
    "# create a count vectorizer TF-DTM object \n",
    "count_vect = CountVectorizer(analyzer='word', token_pattern=r'\\w{1,}')\n",
    "t1 = time.time()\n",
    "count_vect.fit(trainDF['text'])\n",
    "\n",
    "# transform the training and validation data using count vectorizer object\n",
    "xtrain_count =  count_vect.transform(train_x)  # training or calibration sample\n",
    "xvalid_count =  count_vect.transform(valid_x)  # validation or test sample\n",
    "t2 = time.time()\n",
    "\n",
    "print(round(t2 - t1,3), \"secs\")  # ~ 0.15 secs to create DTM. Not bad, eh?\n",
    "print(xtrain_count.shape)\n",
    "print(xvalid_count.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "3b301594",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.055 secs\n",
      "(367, 3323)\n",
      "(123, 3323)\n"
     ]
    }
   ],
   "source": [
    "# word level tf-idf\n",
    "tfidf_vect = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}', max_features=5000)\n",
    "t1 = time.time()\n",
    "tfidf_vect = tfidf_vect.fit(trainDF['text'])\n",
    "xtrain_tfidf =  tfidf_vect.transform(train_x)\n",
    "xvalid_tfidf =  tfidf_vect.transform(valid_x)\n",
    "t2 = time.time()\n",
    "\n",
    "print(round(t2 - t1,3), \"secs\")  # ~ 0.15 secs again\n",
    "print(xtrain_tfidf.shape)\n",
    "print(xvalid_tfidf.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "c935ad9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(classifier, feature_vector_train, label, feature_vector_valid, is_neural_net=False):\n",
    "    # fit the training dataset on the classifier\n",
    "    classifier.fit(feature_vector_train, label)\n",
    "    \n",
    "    # predict the labels on validation dataset\n",
    "    predictions = classifier.predict(feature_vector_valid)\n",
    "    \n",
    "    #print(len(predictions))\n",
    "    predDF = pandas.DataFrame()\n",
    "    predDF['text'] = valid_x\n",
    "    predDF['actual_label'] = valid_y\n",
    "    predDF['model_label'] = predictions\n",
    "    \n",
    "    print(predDF.iloc[:8,])\n",
    "    \n",
    "    if is_neural_net:\n",
    "        predictions = predictions.argmax(axis=-1)\n",
    "    \n",
    "    return metrics.accuracy_score(predictions, valid_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "6ccb4574",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                  text  actual_label  \\\n",
      "397  customer service tried use voucher ride app wo...             0   \n",
      "71   easy inexpensive get held traffic charge pleas...             0   \n",
      "198  guys need customer service phone line contact ...             0   \n",
      "185  main app great integrated ubereats many featur...             0   \n",
      "348  honestly sick tired support agents dont jobs ...             0   \n",
      "427  put 60 card added gift card account go request...             0   \n",
      "308  plain simple never get lyft possible worse eve...             0   \n",
      "450  multiple times ive called wait time one unrea...             1   \n",
      "\n",
      "     model_label  \n",
      "397            0  \n",
      "71             0  \n",
      "198            0  \n",
      "185            0  \n",
      "348            0  \n",
      "427            0  \n",
      "308            0  \n",
      "450            0  \n",
      "\n",
      "\n",
      " 0.01 secs for NB on TF\n",
      "\n",
      "\n",
      "\n",
      "Naive Bayes on DTM accuracy: 0.6260162601626016\n",
      "\n",
      "====================\n",
      "\n",
      "                                                  text  actual_label  \\\n",
      "397  customer service tried use voucher ride app wo...             0   \n",
      "71   easy inexpensive get held traffic charge pleas...             0   \n",
      "198  guys need customer service phone line contact ...             0   \n",
      "185  main app great integrated ubereats many featur...             0   \n",
      "348  honestly sick tired support agents dont jobs ...             0   \n",
      "427  put 60 card added gift card account go request...             0   \n",
      "308  plain simple never get lyft possible worse eve...             0   \n",
      "450  multiple times ive called wait time one unrea...             1   \n",
      "\n",
      "     model_label  \n",
      "397            0  \n",
      "71             0  \n",
      "198            0  \n",
      "185            0  \n",
      "348            0  \n",
      "427            0  \n",
      "308            0  \n",
      "450            0  \n",
      "\n",
      "Naive Bayes on WordLevel TF-IDF accuracy: 0.6991869918699187\n"
     ]
    }
   ],
   "source": [
    "# Naive Bayes on DTM\n",
    "t1 = time.time()\n",
    "accuracy = train_model(naive_bayes.MultinomialNB(), xtrain_count, train_y, xvalid_count)\n",
    "t2 = time.time()\n",
    "print(\"\\n\\n\", round(t2-t1,3), \"secs for NB on TF\\n\\n\")  # 0.01 secs. Fast!\n",
    "print(\"\\nNaive Bayes on DTM accuracy: \"+ str(accuracy))\n",
    "print(\"\\n====================\\n\")\n",
    "\n",
    "# Naive Bayes on Word Level TF IDF Vectors\n",
    "accuracy = train_model(naive_bayes.MultinomialNB(), xtrain_tfidf, train_y, xvalid_tfidf)\n",
    "print(\"\\nNaive Bayes on WordLevel TF-IDF accuracy: \"+ str(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d6c1bac",
   "metadata": {},
   "source": [
    "#### Q5. Recommendations based on the analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fd15ab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# based on the study conducted there is a 69.9% corelation between the ratings given by the passenger and the review given.\n",
    "# The rating is given in the labels to access the relationship between the rating and the comments given by the passengers\n",
    "# Largely the complaints have been around waittimings, customer service and driver related issues\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
